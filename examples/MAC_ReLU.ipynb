{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be797bf-4e07-49f6-b881-dba431e5761e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e32073-2613-4693-8077-6d082fe804a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/daria.cherniuk/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'eval_loss' from 'source.eval' (/beegfs/home/daria.cherniuk/deep-optimization/examples/../source/eval.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AE_ReLU, AE_Sigm\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_train_test_dataloaders\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eval_loss\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'eval_loss' from 'source.eval' (/beegfs/home/daria.cherniuk/deep-optimization/examples/../source/eval.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import USPS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from autograd import grad, jacobian\n",
    "# import autograd.numpy as np\n",
    "\n",
    "from source.models import AE_ReLU, AE_Sigm, AE_ReLU_Small\n",
    "from source.data import get_train_test_dataloaders\n",
    "from source.eval import reconstruction_loss\n",
    "\n",
    "from functools import partial\n",
    "import copy\n",
    "import math\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "# from multiprocessing import Pool\n",
    "import torch.multiprocessing as mp\n",
    "from collections import OrderedDict\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "# torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8ff6a-b5c2-4be5-86cc-d08a09d0cac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae8868-9917-47ee-85e7-71d952f39f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a54e4-7af1-44dc-b6a9-bde68b9137c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_progress(x, z):\n",
    "    rows = 2\n",
    "    cols = 16\n",
    "    x, z = x[:cols], z[:cols]\n",
    "    fig, ax = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(cols, rows))\n",
    "    for i in range(len(x)):\n",
    "        # ax[i//cols, i%cols].imshow(x[i].cpu().reshape([16, 16]), 'gray')\n",
    "        # ax[i//cols, i%cols].set_axis_off()\n",
    "        ax[0, i].imshow(x[i].cpu().reshape([16, 16]), 'gray')\n",
    "        ax[0, i].set_axis_off()\n",
    "    for i in range(len(z)):\n",
    "        # ax[i//cols+2, i%cols].imshow(z[i].detach().cpu().numpy().reshape([16, 16]), 'gray')\n",
    "        # ax[i//cols+2, i%cols].set_axis_off()\n",
    "        ax[1, i].imshow(z[i].detach().cpu().numpy().reshape([16, 16]), 'gray')\n",
    "        ax[1, i].set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc37b3-7e9e-479e-92b7-8ca3f16475d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_train_test_dataloaders('..', 'USPS', \n",
    "                                                       batch_size=7291, \n",
    "                                                       drop_last=False,\n",
    "                                                       num_workers=4)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a2d18-db51-4f63-a44a-36335892adc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a03f9-f841-4cf3-a75e-ae8eaf95466a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import line_search\n",
    "from functools import partial\n",
    "\n",
    "def w_jacob(w_h, layer_idx, n_samples, x, CACHE_RELUS):\n",
    "    J = torch.zeros((n_samples, len(w_h)), device=device)\n",
    "\n",
    "    if layer_idx == 0:\n",
    "        for n in range(n_samples):\n",
    "            # if CACHE_RELUS[layer_idx][n][h] > 0:\n",
    "            if x[n] @ w_h > 0:\n",
    "                J[n, :] = - x[n]\n",
    "    else:\n",
    "        for n in range(n_samples):\n",
    "            if CACHE_RELUS[layer_idx-1][n] @ w_h > 0:\n",
    "                J[n, :] = - CACHE_RELUS[layer_idx-1][n]\n",
    "                \n",
    "    return J\n",
    "\n",
    "def w_residuals(w_h, layer_idx, h, x, CACHE_RELUS):\n",
    "    \n",
    "    if layer_idx == 0:\n",
    "        residuals = CACHE_RELUS[layer_idx][:,h] - torch.relu(torch.einsum('bq,q->b', x, w_h))\n",
    "    elif layer_idx == len(CACHE_RELUS):\n",
    "        residuals = x[:,h] - torch.relu(torch.einsum('bq,q->b', CACHE_RELUS[layer_idx-1], w_h))\n",
    "    else:\n",
    "        residuals = CACHE_RELUS[layer_idx][:,h] - torch.relu(torch.einsum('bq,q->b', CACHE_RELUS[layer_idx-1], w_h))\n",
    "        \n",
    "    return residuals\n",
    "\n",
    "def w_f(w_h, layer_idx, h, x, CACHE_RELUS):\n",
    "    \n",
    "    squares = torch.square(w_residuals(w_h, layer_idx, h, x, CACHE_RELUS))\n",
    "    return squares.sum() / 2\n",
    "\n",
    "def w_step(w_h, layer_idx, h, n_samples, x, CACHE_RELUS):\n",
    "    \n",
    "    residuals = w_residuals(w_h, layer_idx, h, x, CACHE_RELUS)\n",
    "\n",
    "    # Building Jacobian (for each layer and output dim)\n",
    "    J = w_jacob(w_h, layer_idx, n_samples, x, CACHE_RELUS)\n",
    "    \n",
    "    # why not inversion here?\n",
    "    p = torch.linalg.lstsq(J.T@J, - J.T @ residuals, rcond=None)[0]\n",
    "    # p.T @ grad (.T doesn't matter, numpy performs inner product)\n",
    "    descent_inner_prod = (J.T @ residuals) @ p\n",
    "    # try:\n",
    "    #     assert descent_inner_prod <= 0\n",
    "    # except:\n",
    "    #     # pass\n",
    "    #     print(f'layer {layer_idx+1} dim {h} descent(should be <= 0)', descent_inner_prod)\n",
    "\n",
    "    # Line Search with Backtracking\n",
    "    # w_grad = grad(partial(w_f, layer_idx=layer_idx, h=h, \n",
    "    #                       x=x, CACHE_RELUS=CACHE_RELUS))\n",
    "    # def w_grad(w):\n",
    "    #     return J.T @ w_residuals(w, layer_idx, h, x, CACHE_RELUS)\n",
    "    # alpha = line_search(\n",
    "    #     partial(w_f,\n",
    "    #             x=x,\n",
    "    #             layer_idx=layer_idx,\n",
    "    #             h=h,\n",
    "    #             CACHE_RELUS=CACHE_RELUS),\n",
    "    #     w_grad,\n",
    "    #     w_h,\n",
    "    #     p\n",
    "    # )[0]\n",
    "    \n",
    "    \n",
    "    # how to tweak it? \n",
    "    alpha = 10.0\n",
    "    f = w_f(w_h, layer_idx, h, x, CACHE_RELUS)\n",
    "    # First Update\n",
    "    f_new = w_f(w_h + alpha*p, layer_idx, h, x, CACHE_RELUS)\n",
    "    rhs = alpha * line_search_c * descent_inner_prod\n",
    "\n",
    "    max_iter, counter = 40, 0\n",
    "    # Armijo Condition\n",
    "    while f_new - f > rhs and counter < max_iter:\n",
    "        alpha *= line_search_tau\n",
    "        # Update\n",
    "        f_new = w_f(w_h + alpha*p, layer_idx, h, x, CACHE_RELUS)\n",
    "        rhs = alpha * line_search_c * descent_inner_prod\n",
    "\n",
    "        # print(\"Line search armijo: obj func old: %f new: %f diff: %.16f rhs: %.16f, alpha: %f\" % (f, f_new, f_new - f, rhs, alpha))\n",
    "        counter += 1\n",
    "        \n",
    "    # if counter == max_iter:\n",
    "    #     print(f'Layer {layer_idx}: reached maximum number of iterations for line search')\n",
    "    \n",
    "    if alpha is None: alpha = 0.\n",
    "    # print('alpha:', alpha)\n",
    "    \n",
    "    assert not torch.linalg.norm(p).isnan()\n",
    "    assert not torch.linalg.norm(w_h + alpha * p).isnan()\n",
    "        \n",
    "    return w_h + alpha * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc371d-d33b-40b6-ae17-35b01d64ca32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def z_residuals(z_n, h_k, x, WEIGHTS):\n",
    "    \n",
    "    residuals = torch.zeros(h_k[-1], device=device)\n",
    "    start = 0\n",
    "    for layer_idx in range(len(h_k)):\n",
    "        if layer_idx == 0:\n",
    "            residuals[start:h_k[layer_idx]] = z_n[start:h_k[layer_idx]] - torch.relu(torch.einsum('q,hq->h', x, WEIGHTS[layer_idx]))\n",
    "        elif layer_idx == 1:\n",
    "            residuals[start:h_k[layer_idx]] = z_n[start:h_k[layer_idx]] \\\n",
    "                                              - torch.relu(torch.einsum('q,hq->h', z_n[:h_k[layer_idx-1]], \n",
    "                                                                             WEIGHTS[layer_idx]), 0)\n",
    "        elif layer_idx == len(h_k):\n",
    "            residuals[start:h_k[layer_idx]] = x - torch.relu(torch.einsum('q,hq->h', z_n[h_k[layer_idx-2]:h_k[layer_idx-1]], WEIGHTS[layer_idx]))\n",
    "        else:\n",
    "            residuals[start:h_k[layer_idx]] = z_n[start:h_k[layer_idx]] \\\n",
    "                                              - torch.relu(torch.einsum('q,hq->h', z_n[h_k[layer_idx-2]:h_k[layer_idx-1]], \n",
    "                                                                             WEIGHTS[layer_idx]))\n",
    "        start = h_k[layer_idx]\n",
    "    return residuals\n",
    "\n",
    "def z_jacob(z_n, mu, h_k, WEIGHTS):\n",
    "    J = torch.zeros((h_k[-1], h_k[-1]), device=device)\n",
    "    for h in range(h_k[-1]):\n",
    "        J[h, h] = math.sqrt(mu)\n",
    "        \n",
    "        \n",
    "        for layer_idx in range(len(h_k)):\n",
    "            # find layer that this coordinate belongs to\n",
    "            if h < h_k[layer_idx]: break\n",
    "        # if this is a coordinate from the first layer\n",
    "        # then the residual only depends on z_1\n",
    "        if layer_idx == 0: continue\n",
    "\n",
    "        # coordinate relatively to the current layer\n",
    "        n_feature = h-h_k[layer_idx-1]\n",
    "        assert n_feature >= 0 and n_feature < WEIGHTS[layer_idx].shape[0]\n",
    "\n",
    "        if layer_idx == 1:\n",
    "            start, finish = 0, h_k[layer_idx-1]\n",
    "        elif layer_idx > 1:\n",
    "            start, finish = h_k[layer_idx-2], h_k[layer_idx-1]\n",
    "            \n",
    "        # of if WEIGHTS[layer_idx][n_feature] @ z_k > 0?\n",
    "        if z_n[h] > 0:\n",
    "            J[h, start:finish] = - WEIGHTS[layer_idx][n_feature]\n",
    "            # Quadratic Penalty\n",
    "            if layer_idx != len(h_k)-1:\n",
    "                J[h, start:finish] *= mu\n",
    "                \n",
    "    return J\n",
    "\n",
    "def z_f(z_n, mu, h_k, x, WEIGHTS):\n",
    "    squares = torch.square(z_residuals(z_n, h_k, x, WEIGHTS))\n",
    "    return squares[:h_k[-2]+1].sum() * mu / 2 + squares[h_k[-2]+1:].sum() / 2\n",
    "\n",
    "def z_step(z_n, n_sample, mu, h_k, x, WEIGHTS):\n",
    "    \n",
    "    # Calculating Residuals\n",
    "    residuals = z_residuals(z_n, h_k, x, WEIGHTS)\n",
    "    assert len(residuals) == h_k[-1]\n",
    "    \n",
    "    # is it needed? \n",
    "    mu_mult = torch.ones_like(residuals)\n",
    "    mu_mult[:h_k[-2]] = math.sqrt(mu)\n",
    "    residuals = mu_mult * residuals\n",
    "\n",
    "    # Building Jacobian for each sample in dataset\n",
    "    J = z_jacob(z_n, mu, h_k, WEIGHTS)\n",
    "\n",
    "    # Descent Direction\n",
    "    p = torch.linalg.lstsq(J.T@J, - J.T @ residuals, rcond=None)[0]\n",
    "    # p.T @ grad (.T doesn't matter, numpy performs inner product)\n",
    "    descent_inner_prod = (J.T @ residuals) @ p\n",
    "    try:\n",
    "        assert descent_inner_prod <= 0\n",
    "    except:\n",
    "        print(f'sample {n_sample} descent direction should be <= 0', descent_inner_prod)\n",
    "\n",
    "    # Line Search with Backtracking\n",
    "    # p_ = torch.zeros(h_k[-1])\n",
    "    # p_[:h_k[-2]] = p\n",
    "    alpha = 1.0\n",
    "    f = z_f(z_n, mu, h_k, x, WEIGHTS)\n",
    "    f_new = z_f(z_n + alpha * p, mu, h_k, x, WEIGHTS)\n",
    "    rhs = alpha * line_search_c * descent_inner_prod\n",
    "\n",
    "    max_iter, counter = 40, 0\n",
    "    # Armijo Condition\n",
    "    while f_new - f > rhs and counter < max_iter:\n",
    "        # step update\n",
    "        alpha *= line_search_tau\n",
    "\n",
    "        f_new = z_f(z_n + alpha * p, mu, h_k, x, WEIGHTS)\n",
    "        rhs = alpha * line_search_c * descent_inner_prod\n",
    "\n",
    "        # print(\"Line search armijo: obj func old: %f new: %f diff: %.16f rhs: %.16f, alpha: %f\" % (f, f_new, f_new - f, rhs, alpha))\n",
    "        counter += 1\n",
    "        \n",
    "    # if counter == max_iter:\n",
    "    #     print(f'Layer {layer_idx}: reached maximum number of iterations for line search')\n",
    "     \n",
    "    return J, z_n + alpha * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e9db9-c846-4cb3-aabd-30b9653fe401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader, test_loader = get_train_test_dataloaders('..', 'USPS', \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       drop_last=False,\n",
    "                                                       num_workers=4)\n",
    "for x, _ in train_loader:\n",
    "    x = x.to(device)\n",
    "    break\n",
    "\n",
    "line_search_c = pow(10,-4)\n",
    "# backtracking multiplier\n",
    "line_search_tau = 0.5\n",
    "\n",
    "# Quadratic Penalty multiplier \n",
    "mu = 1.0\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "loss_hist = []\n",
    "\n",
    "# creating model\n",
    "model = AE_ReLU(bias=False).to(device)\n",
    "loss = eval_loss(model, test_loader, loss_function, device=device)\n",
    "loss_hist.append(loss)\n",
    "print(f'Random-initialized model loss: {loss}')\n",
    "\n",
    "WEIGHTS = [\n",
    "    model.encoder[0].weight.detach(),\n",
    "    model.encoder[2].weight.detach(),\n",
    "    model.encoder[4].weight.detach(),\n",
    "    model.decoder[0].weight.detach(),\n",
    "    model.decoder[2].weight.detach(),\n",
    "    model.decoder[4].weight.detach(),\n",
    "]\n",
    "\n",
    "# all output shapes\n",
    "h_k = [0]\n",
    "for W in WEIGHTS:\n",
    "    h_k.append(W.shape[0] + h_k[-1])\n",
    "h_k = h_k[1:-1]\n",
    "\n",
    "# # registering hooks to cache activations\n",
    "# CACHE_RELUS = {}\n",
    "\n",
    "# def cache_relu_hook(idx, module, input, output):\n",
    "#     CACHE_RELUS[idx] = output.detach().cpu()\n",
    "\n",
    "# i = 0 \n",
    "# for m in model.modules():\n",
    "#     if isinstance(m, nn.ReLU):\n",
    "#         handle = m.register_forward_hook(partial(cache_relu_hook, i))\n",
    "#         i += 1\n",
    "        \n",
    "# # caching activations\n",
    "# with torch.no_grad():\n",
    "#     for x, _ in train_loader:\n",
    "#         x = x.to(device)\n",
    "#         _ = model(x)\n",
    "#         x = x.detach().cpu()\n",
    "        \n",
    "# # Removing Hooks\n",
    "# for m in model.modules():\n",
    "#     if isinstance(m, nn.ReLU):\n",
    "#         m._forward_hooks = OrderedDict()\n",
    "\n",
    "n_samples = train_loader.batch_size\n",
    "# init z_n randomly\n",
    "CACHE_RELUS = {}\n",
    "for i, w in enumerate(WEIGHTS[:-1]):\n",
    "    # CACHE_RELUS[i] = np.random.uniform(-0.5, 0.5, size=(n_samples, w.shape[0]))\n",
    "    CACHE_RELUS[i] = torch.rand((n_samples, w.shape[0]), device=device) - 0.5 \n",
    "        \n",
    "def _get_n_sample_relus(n_sample, cache):\n",
    "    n_sample_relus = torch.zeros(h_k[-1])\n",
    "    start = 0\n",
    "    for layer_idx in range(len(h_k)):\n",
    "        n_sample_relus[start:h_k[layer_idx]] = cache[layer_idx][n_sample]\n",
    "        start = h_k[layer_idx]\n",
    "    return n_sample_relus\n",
    "\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for epoch in range(10):\n",
    "            \n",
    "        # W-Step\n",
    "        WEIGHTS_COPY = copy.deepcopy(WEIGHTS)\n",
    "\n",
    "        for layer_idx in tqdm(range(len(WEIGHTS))):\n",
    "\n",
    "            for h in range(WEIGHTS[layer_idx].shape[0]):\n",
    "                for it in range(3):\n",
    "                    w_h = WEIGHTS[layer_idx][h]\n",
    "                    new_w_h = w_step(w_h, layer_idx, h, n_samples, x, CACHE_RELUS)\n",
    "                    # rel_diff = torch.linalg.norm(new_w_h-w_h) / torch.linalg.norm(w_h)\n",
    "                    # print(f'layer_idx {layer_idx}, h: {h}, it: {it}, REL_DIFF: {rel_diff}')\n",
    "                    WEIGHTS[layer_idx][h] = new_w_h\n",
    "                    \n",
    "                    assert not torch.linalg.norm(new_w_h).isnan()\n",
    "                    assert not torch.linalg.norm(w_h).isnan()\n",
    "                    assert not torch.linalg.norm(new_w_h - w_h).isnan()\n",
    "                    \n",
    "        # Checking Norm Difference\n",
    "        for layer_idx in range(len(WEIGHTS)):\n",
    "            print(f'Layer {layer_idx}, '\n",
    "                  f'Weights Diff Norm (%): {torch.linalg.norm(WEIGHTS[layer_idx]-WEIGHTS_COPY[layer_idx]) / torch.linalg.norm(WEIGHTS_COPY[layer_idx])}')\n",
    "        \n",
    "        # Loading weights back to model to compute eval loss\n",
    "        model.encoder[0].weight.data = WEIGHTS[0]\n",
    "        model.encoder[2].weight.data = WEIGHTS[1]\n",
    "        model.encoder[4].weight.data = WEIGHTS[2]\n",
    "        model.decoder[0].weight.data = WEIGHTS[3]\n",
    "        model.decoder[2].weight.data = WEIGHTS[4]\n",
    "        model.decoder[4].weight.data = WEIGHTS[5]\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Loss on eval dataset\n",
    "        loss = eval_loss(model, test_loader, loss_function, device=device)\n",
    "        loss_hist.append(loss)\n",
    "        print(f'Epoch {epoch+1}, Eval loss: {loss}')\n",
    "        \n",
    "        # # recalculating activations after weights update\n",
    "        # for x, _ in train_loader:\n",
    "        #     x = x.to(device)\n",
    "        #     _ = model(x)\n",
    "        #     x = x.cpu()\n",
    "    \n",
    "        # Z-Step\n",
    "        CACHE_RELUS_COPY = copy.deepcopy(CACHE_RELUS)\n",
    "        \n",
    "        # process for each sample\n",
    "        for n_sample in tqdm(range(n_samples)):\n",
    "            for it in range(1):\n",
    "                z_n = _get_n_sample_relus(n_sample, CACHE_RELUS)\n",
    "                J, new_z_n = z_step(z_n, n_sample, mu, h_k, x[n_sample], WEIGHTS)\n",
    "                start = 0\n",
    "                for layer_idx in range(len(h_k)):\n",
    "                    CACHE_RELUS[layer_idx][n_sample] = new_z_n[start:h_k[layer_idx]]\n",
    "                    start = h_k[layer_idx]\n",
    "                    \n",
    "                # rel_diff = torch.linalg.norm(new_z_n - z_n) / torch.linalg.norm(z_n)\n",
    "\n",
    "        # Computing Relus Norm Difference\n",
    "        for layer_idx in range(len(CACHE_RELUS)):\n",
    "            print(f'Layer {layer_idx} Activations Diff Norm (%): '\n",
    "                  f'{torch.linalg.norm(CACHE_RELUS[layer_idx]-CACHE_RELUS_COPY[layer_idx]) / torch.linalg.norm(CACHE_RELUS_COPY[layer_idx])}')\n",
    "            \n",
    "        # Updating Quandratic Penalty Multiplier\n",
    "        if it > 2 and abs(loss_hist[-2] - loss_hist[-1]) < 1e-3:\n",
    "            mu *= 10\n",
    "            print(f'New MU: {mu}')\n",
    "        \n",
    "        if epoch > 0 and epoch % 2 == 0: \n",
    "            with torch.no_grad():\n",
    "                y = model(x[:16].cuda())\n",
    "            plot_progress(x[:16], y)\n",
    "           \n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3676ad-5925-4a81-9495-3b8e63ecb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3776d8-58ba-43ac-88f8-dbd8c94b8d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.spy(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d262e-5007-4eb4-970d-4e3013a20f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x.device, w_h.device, CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a98db-8dce-454b-aaec-24020268abf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eve3",
   "language": "python",
   "name": "eve3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
